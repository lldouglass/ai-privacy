# Outcome 8: Deployer of High-Risk AI System

## Overview

This outcome applies to any **deployer** (person doing business in Colorado that deploys a high-risk AI system) that uses a **high-risk artificial intelligence system** to make or assist in making consequential decisions.

Deployers of high-risk AI systems are subject to specific obligations under § 6-1-1703 to protect consumers from algorithmic discrimination.

## Effective Date

**February 1, 2026** — Deployer obligations under the Colorado AI Act take effect on this date.

## Exemptions

Some deployers may be exempt. See **Outcome 2: Exempt Deployer** for:
- Small deployer exemptions (size-based thresholds)
- Regulated industry exemptions (insurance, banking)

## Primary Duty of Care

**§ 6-1-1703(1) — Duty of Care**

A deployer shall use **reasonable care** to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination arising from the deployer's **intended and contracted uses** of the high-risk artificial intelligence system.

### Rebuttable Presumption of Reasonable Care

A deployer maintains a **rebuttable presumption** that the deployer has used reasonable care to protect consumers if the deployer complies with the requirements specified in § 6-1-1703.

This means: If you follow all the statutory requirements below, you are presumed to have met your duty of care (though this presumption can be challenged).

## Required Deployer Obligations

### 1. Risk Management Policy and Program

**§ 6-1-1703(2) — Risk Management Requirements**

A deployer shall implement a **risk management policy and program** for any high-risk artificial intelligence system that the deployer deploys.

**Required components:**

#### (a) Documented Policies and Procedures
Documentation and implementation of policies, procedures, and practices to manage known or reasonably foreseeable risks of algorithmic discrimination

#### (b) Regular Risk Identification
Ongoing processes to regularly identify, document, and mitigate known or reasonably foreseeable risks

#### (c) Annual Review and Updates
Annual review and update of the risk management policy and program

**Purpose**: To create a systematic, documented approach to preventing and addressing algorithmic discrimination throughout the deployment lifecycle.

### 2. Impact Assessment

**§ 6-1-1703(3) — Impact Assessment Requirements**

A deployer shall complete an **impact assessment** for each high-risk artificial intelligence system **prior to deploying** the system.

**Required contents of impact assessment:**

#### (a) Purpose and Use Statement
A statement of:
- The purpose of the high-risk AI system
- Its intended use cases
- Its intended benefits
- Its deployment context

#### (b) Discrimination Risk Analysis
An analysis of whether the deployment of the high-risk AI system poses any known or reasonably foreseeable risks of algorithmic discrimination

#### (c) Data Categories and Management
A description of:
- The categories of data that the high-risk AI system will process
- How the data will be collected
- How the data will be used
- How the data will be protected
- How the data will be retained

#### (d) Risk Management Implementation Description
A description of the deployer's implementation of the risk management policy and program

#### (e) Performance Metrics
Metrics used to evaluate the high-risk AI system's performance in relation to algorithmic discrimination

**Disclosure of impact assessment:**
- Make available to the **developer** that developed the high-risk AI system
- Make available to the **Attorney General** upon request

### 3. Public Summary of Impact Assessment

**§ 6-1-1703(4) — Public Transparency Requirement**

A deployer shall make available on the deployer's **website** a **public summary** of each impact assessment.

**Required contents of public summary:**

#### (a) Intended Use
The intended use of the high-risk AI system

#### (b) Risk Management Approach
How the deployer manages known or reasonably foreseeable risks of algorithmic discrimination

**Purpose**: To provide public transparency about how high-risk AI systems are being used and how discrimination risks are managed.

### 4. Consumer Notice and Rights

**§ 6-1-1703(5) — Consumer Disclosure Requirements**

When a deployer makes, or is a substantial factor in making, a **consequential decision** concerning a consumer, the deployer shall provide the consumer with:

#### (a) Statement of AI Use
A statement that a high-risk AI system was used in the decision-making

#### (b) Purpose Information
Information about the purpose of the high-risk AI system

#### (c) Decision Nature
The nature of the consequential decision

#### (d) Contact Information
Contact information for the deployer where the consumer can obtain additional information

#### (e) Opt-Out Rights
A statement explaining the right to opt out of the processing, where applicable

#### (f) Appeal and Review Rights
A statement explaining the right to:
- Appeal the decision
- Seek human review of the decision

### 5. Adverse Decision Requirements

**§ 6-1-1703(6) — Additional Requirements for Adverse Decisions**

If a deployer makes an **adverse consequential decision** concerning a consumer that is based in whole or in part on the output of a high-risk AI system, the deployer shall provide:

#### (a) Statement of Principal Reasons
An explanation of the principal reason or reasons for the adverse decision, including:
- The data or data source that was a significant factor in the decision

#### (b) Right to Correct Incorrect Data
An opportunity for the consumer to correct any incorrect personal data that was processed by the high-risk AI system

#### (c) Right to Appeal with Human Review
An opportunity for the consumer to appeal the adverse decision, which must include:
- A process for the consumer to contest the adverse decision
- A process to provide **human review** of the decision, **where technically feasible**
- Information about how to submit an appeal

**Purpose**: To provide meaningful recourse for consumers who receive adverse decisions based on AI systems.

### 6. Notification of Attorney General

**§ 6-1-1703(7) — Discovered Discrimination Reporting**

When a deployer discovers that its use of a high-risk AI system has resulted in **algorithmic discrimination**, the deployer shall:

#### Notify the Attorney General
- **Timing**: As soon as reasonably practicable after discovery
- **Content**: Information about the nature and extent of the algorithmic discrimination
- **Purpose**: To enable regulatory oversight and potential intervention

## Trade Secret and Security Protections

**§ 6-1-1703(8) — Protection of Proprietary Information**

The requirements of § 6-1-1703 do **not** require a deployer to disclose:

- Trade secrets
- Information protected from disclosure by state or federal law
- Information that would create a security risk

**Exception to withholding**: When withholding information from a consumer (e.g., in an adverse decision explanation), the deployer shall provide:
- Notice that certain information is being withheld
- A statement of the basis for withholding the information

## Compliance with Risk Management Frameworks

**§ 6-1-1703(9) — Safe Harbor**

A deployer is deemed in compliance if the deployer complies with a **nationally or internationally recognized risk management framework** for AI systems, as may be specified by the Colorado Attorney General through rulemaking.

**Examples of frameworks that may qualify:**
- NIST AI Risk Management Framework
- ISO/IEC standards for AI systems
- Other frameworks recognized by the Attorney General

## Enforcement and Penalties

**§ 6-1-1706 — Enforcement**

Violations of deployer obligations constitute a **deceptive trade practice** under the Colorado Consumer Protection Act (§ 6-1-105).

**Enforcement authority:**
- Colorado Attorney General has exclusive enforcement authority
- Civil penalties may apply
- Injunctive relief may be ordered

**Private right of action**: Limited private rights of action may be available under certain circumstances after Attorney General enforcement.

## Relationship to Other Outcomes

### If you are ONLY a deployer (not also a developer):
- You are subject to § 6-1-1703 obligations only
- You are NOT subject to § 6-1-1702 developer obligations
- **Outcome**: Outcome 8 (Deployer of High-Risk AI System)

### If you are BOTH a developer AND a deployer:
- You are subject to BOTH § 6-1-1702 and § 6-1-1703
- You must comply with all developer obligations AND all deployer obligations
- **Outcome**: Outcome 9 (Both Developer and Deployer)

### If you qualify for an exemption:
- Check **Outcome 2** (Exempt Deployer) for size-based or industry-specific exemptions

## Practical Compliance Steps

To comply with deployer obligations:

1. **Before deployment:**
   - Complete a comprehensive impact assessment
   - Implement risk management policy and program
   - Publish public summary on your website

2. **During deployment:**
   - Provide consumer notices when making consequential decisions
   - Monitor for algorithmic discrimination
   - Maintain and update risk management practices annually

3. **For adverse decisions:**
   - Provide explanation of principal reasons
   - Offer opportunity to correct data
   - Establish appeal process with human review

4. **If discrimination is discovered:**
   - Notify Attorney General promptly
   - Take corrective action
   - Update impact assessment and risk management program

5. **Documentation:**
   - Maintain records of all compliance activities
   - Keep impact assessments current
   - Document risk management reviews and updates

6. **Consider safe harbor:**
   - Adopt a recognized risk management framework
   - Document framework compliance

## Related Provisions

- § 6-1-1703 — Complete deployer obligations
- § 6-1-1704 — Consumer disclosure requirements (additional details)
- **Deployer** (§ 6-1-1701(5)) — Definition
- **High-Risk Artificial Intelligence System** (§ 6-1-1701(9)) — What triggers these obligations
- **Algorithmic Discrimination** (§ 6-1-1701(1)) — The primary risk deployers must address
- **Impact Assessment** (§ 6-1-1701(8)) — Detailed definition
- **Risk Management Policy and Program** (§ 6-1-1701(14)) — Detailed definition
- Outcome 2 — Exempt deployer provisions
- Outcome 7 — Developer obligations (for understanding what your vendors must do)
- Outcome 9 — Combined developer and deployer obligations

