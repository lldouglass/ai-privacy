# Outcome 7: Developer of High-Risk AI System

## Overview

This outcome applies to any **developer** (person doing business in Colorado that develops or intentionally and substantially modifies an AI system) that creates or substantially modifies a **high-risk artificial intelligence system**.

Developers of high-risk AI systems are subject to specific obligations under § 6-1-1702 to protect consumers from algorithmic discrimination.

## Primary Duty of Care

**§ 6-1-1702(1) — Duty of Care**

A developer shall use **reasonable care** to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination arising from the **intended and contracted uses** of any high-risk artificial intelligence system that the developer develops.

### Rebuttable Presumption of Reasonable Care

A developer maintains a **rebuttable presumption** that the developer has used reasonable care to protect consumers if the developer complies with the requirements specified in § 6-1-1702.

This means: If you follow all the statutory requirements below, you are presumed to have met your duty of care (though this presumption can be challenged).

## Required Developer Obligations

### 1. Documentation and Disclosure to Deployers

**§ 6-1-1702(2) — Documentation Requirements**

A developer of a high-risk AI system shall make **documentation and a statement** available to any deployer **before or contemporaneously with** the deployer's use of the high-risk AI system.

**Required documentation contents:**

#### (a) General Statement of Uses
A general statement describing:
- The reasonably foreseeable uses of the high-risk AI system
- Known harmful or inappropriate uses of the high-risk AI system

#### (b) Training Data Summary
A summary of the type of data used to train the high-risk AI system

#### (c) Performance Evaluation Description
A description of:
- How the high-risk AI system was evaluated for performance and mitigation of algorithmic discrimination
- Limitations of the evaluation
- Metrics the deployer may use to evaluate performance

#### (d) Monitoring Data Requirements
A description of the data that the deployer must provide to the developer to monitor the high-risk AI system

#### (e) Data Input Overview
An overview of the type of data that the deployer should use with the high-risk AI system

#### (f) Known Limitations and Risks
Any known or reasonably foreseeable limitations of the high-risk AI system, including:
- Known or reasonably foreseeable risks of algorithmic discrimination

### 2. Notification of Discovered Risks

**§ 6-1-1702(3) — Updates and Modifications**

When a developer becomes aware that a deployed high-risk AI system has produced **algorithmic discrimination** or otherwise poses an **unreasonable risk** to consumers, the developer shall:

#### (a) Provide Notice to Deployers
- **Timing**: Within **90 days** of discovery
- **Recipients**: All deployers using the affected high-risk AI system

#### (b) Provide Risk Documentation
- Documentation of the nature of the algorithmic discrimination or unreasonable risk
- Information to help deployers understand and address the risk

### 3. Testing and Validation

**§ 6-1-1702(4) — Testing Requirements**

A developer shall implement **appropriate procedures and processes** to test a high-risk AI system to identify known or reasonably foreseeable risks of algorithmic discrimination.

**Testing requirements may include:**

#### (a) Pre-Deployment Testing
Testing before the high-risk AI system is deployed for use

#### (b) Representative Data Testing
Using representative, reasonably up-to-date data sets for testing

**Purpose**: To identify and mitigate risks of algorithmic discrimination before the system is made available to deployers.

### 4. Additional Transparency Obligations

**§ 6-1-1702(5) — General Transparency**

Developers must provide sufficient information to enable deployers to:
- Understand how the high-risk AI system operates
- Assess its suitability for their intended use
- Comply with their own deployer obligations under § 6-1-1703

## Trade Secret Protection

**§ 6-1-1702(6) — Protection of Proprietary Information**

The requirements of § 6-1-1702 do **not** require a developer to disclose:

- Trade secrets
- Information protected from disclosure by state or federal law
- Information that would create a security risk

**Balance**: Developers must provide sufficient information for deployer compliance while protecting legitimate proprietary interests.

## What "Reasonable Care" Means

The duty to use "reasonable care" is evaluated based on:

1. **Known risks**: Risks the developer actually knows about
2. **Reasonably foreseeable risks**: Risks the developer should anticipate based on the system's design and intended use
3. **Intended and contracted uses**: The uses contemplated in agreements with deployers and the system's design purpose
4. **Industry standards**: Compliance with nationally or internationally recognized risk management frameworks

## Compliance with Risk Management Frameworks

**§ 6-1-1702(7) — Safe Harbor**

A developer is deemed in compliance if the developer complies with a **nationally or internationally recognized risk management framework** for AI systems, as may be specified by the Colorado Attorney General through rulemaking.

**Examples of frameworks that may qualify:**
- NIST AI Risk Management Framework
- ISO/IEC standards for AI systems
- Other frameworks recognized by the Attorney General

## Enforcement and Penalties

**§ 6-1-1706 — Enforcement**

Violations of developer obligations constitute a **deceptive trade practice** under the Colorado Consumer Protection Act (§ 6-1-105).

**Enforcement authority:**
- Colorado Attorney General has exclusive enforcement authority
- Civil penalties may apply
- Injunctive relief may be ordered

**Private right of action**: Limited private rights of action may be available under certain circumstances after Attorney General enforcement.

## Relationship to Other Outcomes

### If you are ONLY a developer (not also a deployer):
- You are subject to § 6-1-1702 obligations only
- You are NOT subject to § 6-1-1703 deployer obligations
- **Outcome**: Outcome 7 (Developer of High-Risk AI System)

### If you are BOTH a developer AND a deployer:
- You are subject to BOTH § 6-1-1702 and § 6-1-1703
- You must comply with all developer obligations AND all deployer obligations
- **Outcome**: Outcome 9 (Both Developer and Deployer)

## Practical Compliance Steps

To comply with developer obligations:

1. **Implement pre-deployment testing** using representative data sets
2. **Create comprehensive documentation** covering all required elements in § 6-1-1702(2)
3. **Establish monitoring systems** to detect algorithmic discrimination or unreasonable risks
4. **Develop notification procedures** to alert deployers within 90 days of discovering risks
5. **Provide ongoing support** to deployers to enable their compliance
6. **Consider adopting** a recognized risk management framework for safe harbor compliance
7. **Document all compliance activities** to support the rebuttable presumption of reasonable care

## Related Provisions

- § 6-1-1702 — Complete developer obligations
- **Developer** (§ 6-1-1701(4)) — Definition
- **High-Risk Artificial Intelligence System** (§ 6-1-1701(9)) — What triggers these obligations
- **Algorithmic Discrimination** (§ 6-1-1701(1)) — The primary risk developers must address
- Outcome 8 — Deployer obligations (for understanding what your customers must do)
- Outcome 9 — Combined developer and deployer obligations

